# StepDetails messages 
hdp.clusterdescription.description=Cluster Description
hdp.clusterdescription.formTitle=Provide details about your cluster
hdp.clusterdescription.formDescription=Complete the fields below to describe your new {name} cluster. Required fields are indicated with an asterisk (*).
hdp.clusterparameters.description=Cluster Parameters
hdp.clusterparameters.formTitle=Provide details about the cluster size
hdp.clusterparameters.formDescription=Please provide details about your cluster size. Required fields are indicated with an asterisk (*). 
hdp.dataautomation.description=Data Automation
hdp.dataautomation.formTitle=Provide information about the object store
hdp.dataautomation.formDescription=Please provide details of your data automation
hdp.securehadoop.description=Secure Hadoop
hdp.securehadoop.formTitle=Secure Hadoop
hdp.securehadoop.formDescription=Please provide details of your Secure Hadoop
hdp.gatewayparameters.description=Gateway Parameters
hdp.gatewayparameters.formTitle=Dedicated VPN Gateway Parameters
hdp.gatewayparameters.formDescription=Provide details about your dedicated VPN gateway. This step is optional. You may skip this step and enter the gateway parameters after the cluster is provisioned. Required fields are indicated with an asterisk (*).
hdp.reviewdetails.description=Review Details
hdp.reviewdetails.formTitle=Review Details
hdp.reviewdetails.formDescription=Please Review your details for {name} ~ Note: After the environment is created, click on the cluster name to view the password information.

bi30.clusterdescription.description=Cluster Description
bi30.clusterdescription.formTitle=Provide details about your cluster
bi30.clusterdescription.formDescription=Complete the fields below to describe your new {name} cluster. Required fields are indicated with an asterisk (*).
bi30.clusterparameters.description=Cluster Parameters
bi30.clusterparameters.formTitle=Provide details about the cluster size
bi30.clusterparameters.formDescription=Please provide details about your cluster size. Required fields are indicated with an asterisk (*). 
bi30.dataautomation.description=Data Automation
bi30.dataautomation.formTitle=Provide information about the object store
bi30.dataautomation.formDescription=Please provide details of your data automation
bi30.securehadoop.description=Secure Hadoop
bi30.securehadoop.formTitle=Secure Hadoop
bi30.securehadoop.formDescription=Please provide details of your Secure Hadoop
bi30.reviewdetails.description=Review Details
bi30.reviewdetails.formDescription=Please Review your details for {name} ~ Note: After the environment is created, click on the cluster name to view the password information.
bi30.reviewdetails.formTitle=Review Details

# NodeConfigurations
hdp.header.nodeSize=Virtual Node Size
hdp.header.specification=Specifications
hdp.header.dataBandwidth=Monthly Bandwidth Allowance
hdp.header.usedFor=Used For
hdp.small.nodeSize=Small
hdp.small.specification=&bull; 2 Physical Cores (4 vCPUs)\r\n&bull; 19GB RAM\r\n&bull; 2 X 3.5TB direct attached SATA disks\r\n&bull; Shared 10GBPs Ethernet
hdp.small.dataBandwidth=&bull; 5TB per node over public interface\r\n&bull; Unlimited over private interface
hdp.small.usedFor=Data extraction, transformation, file processing, search
hdp.large.nodeSize=Large
hdp.large.specification=&bull;10 Physical Cores (20 vCPUs)\r\n&bull; 96GB RAM\r\n&bull; 10 X 3.5TB direct attached SATA disks\r\n&bull; Dedicated 10GBPs Ethernet
hdp.large.dataBandwidth=&bull; 20TB per node over public interface\r\n&bull; Unlimited over private interface
hdp.large.usedFor=External data ingest, data warehouse, advanced analytics
bi30.small.nodeSize=Small
bi30.small.specification=&bull; 2 Physical Cores (4 vCPUs)\r\n&bull; 19GB RAM\r\n&bull; 2 X 3.5TB direct attached SATA disks\r\n&bull; Shared 10GBPs Ethernet
bi30.small.dataBandwidth=&bull; 5TB per node over public interface\r\n&bull; Unlimited over private interface
bi30.small.usedFor=Data extraction, transformation, file processing, search

#formfields:- label , description

hdp.clusterName=Cluster Name
hdp.clusterName.description=Enter Cluster Name
hdp.clusterDescription=Cluster Description
hdp.clusterDescription.description=Enter Cluster Description
hdp.dataNode=Data Nodes
hdp.dataNode.description=This cluster allows you to have from 1 up to n-1 data nodes where n is the quota for your cluster. Click on Info Icon to learn more about Data Nodes.
hdp.dataNode.helpDescription=A data node runs compute services like Data Node, HBase Region Server, Task Tracker etc. Data nodes provide all of their local storage to HDFS.
hdp.masterNode=Master Node
hdp.masterNode.description=This cluster requires 1 and only 1 master node. Click on Info Icon to learn more about Master Nodes.
hdp.masterNode.helpDescription=A master node runs management services like Admin Console, NameNode, Secondary NameNode, JobTracker, HCatalog, etc. If the cluster has / provides access to the Internet, the master node is provisioned with a public interface.
hdp.nodeConfiguration=Show / Hide node configuration
hdp.nodeConfiguration.description=Show / Hide node configuration
hdp.edgeNodes=Edge Nodes
hdp.edgeNodes.description=This cluster allows you to have from 0 up to w edge nodes where w is the number of worker nodes for your cluster. Click on Info Icon to learn more about Edge Nodes.
hdp.edgeNodes.helpDescription=An edge node is a worker node that is provisioned with a public interface so that the cluster has additional nodes (other than the master node) with public domain access. These nodes can be used to ingest data at scale or from different data sources.
hdp.cpeLocationName=Cluster Location
hdp.cpeLocationName.description=Select Cluster Location
hdp.importData=Import Data to HDFS
hdp.importData.description=Select Import Data to HDFS from \r\n1)No\r\n2)From Softlayer Object
hdp.DATA_IN_USERNAME=Object Store Username
hdp.DATA_IN_USERNAME.description=Enter Object Store Username
hdp.DATA_IN_APIKEY=Object Store API Key
hdp.DATA_IN_APIKEY.description=Specify the API Key of the object store account
hdp.DATA_IN_LOCATION=Data Center
hdp.DATA_IN_LOCATION.description=Select Data Center Location
hdp.DATA_IN_CONTAINER=Object Store Container
hdp.DATA_IN_CONTAINER.description=Select HDFS Object Store Container
hdp.DATA_IN_PATH=Object Store Path
hdp.DATA_IN_PATH.description=Enter the source path for your transfer. \r\n1) To transfer all content from the container, enter * \r\n2) To transfer a specific object from the container, enter objectname \r\n3) To transfer all objects from a folder or sub-folder within the container, enter folder/* or folder/subfolder/*, etc. \r\n4) To transfer a specific object from a folder or sub-folder, enter folder/objectname or folder/subfolder/objectname, etc.
hdp.DATA_IN_DESTINATION=HDFS Directory
hdp.DATA_IN_DESTINATION.description=Enter the target directory for your transfer.  A new directory will be created, and the transfer will go to this directory.  If you enter a slash (/) only, the transfer will go to the parent directory \r\n\r\nExamples\:\r\n/\r\n/directoryname\r\n/directoryname/subdirectoryname

## fields for From Transfer
TRANSFER.FROM.hdp.DATA_IN_PATH=Object Store Path
TRANSFER.FROM.hdp.DATA_IN_PATH.description=Enter the source path for your transfer. \r\n1) To transfer all content from the container, enter * \r\n2) To transfer a specific object from the container, enter objectname \r\n3) To transfer all objects from a folder or sub-folder within the container, enter folder/* or folder/subfolder/*, etc. \r\n4) To transfer a specific object from a folder or sub-folder, enter folder/objectname or folder/subfolder/objectname, etc.
TRANSFER.FROM.hdp.DATA_IN_DESTINATION=HDFS Directory
TRANSFER.FROM.hdp.DATA_IN_DESTINATION.description=Enter the target directory for your transfer.  You may enter a new directory or an existing one.  A new directory will be created with the name you enter if it doesn't already exist. \r\n\r\nExamples\: \r\n/directoryname \r\n/directoryname/subdirectoryname

## fields for To Transfer
TRANSFER.TO.hdp.DATA_IN_PATH=Object Store Path
TRANSFER.TO.hdp.DATA_IN_PATH.description=Enter the source path for your transfer. \r\n\r\n1) To transfer directly to the container, enter / \r\n2) To transfer to a new directory, enter /newDirectoryName
TRANSFER.TO.hdp.DATA_IN_DESTINATION=HDFS Path
TRANSFER.TO.hdp.DATA_IN_DESTINATION.description=Enter the source path for your transfer.  It is not recommended to specify a wildcard at the directory level if that directory contains any subdirectories.  Instead, specify the full path. \r\n\r\n1) To transfer all objects from a directory, enter /directory/* \r\n2) To transfer a specific object from a directory, enter /directory/objectname \r\n3) To transfer all objects from a subdirectory, enter /directory/subdirectory/* \r\n4) To transfer a specific object from a subdirectory, enter /directory/subdirectory/objectname

hdp.dedicatedVPN=Specify gateway parameters for this cluster?
hdp.dedicatedVPN.description=Select if dedicated VPN required
hdp.USE_SDFS=Use Secure Hadoop with this cluster?
hdp.USE_SDFS.description=Specify here whether you want to use Secure Hadoop Data Encryption with this cluster.
hdp.SDFS_USERNAME=Object Storage Username
hdp.SDFS_USERNAME.description=To use Secure Hadoop Data Encryption with your Hadoop cluster, please provide the credentials for your Object Storage account.
hdp.SDFS_APIKEY=Object Storage API Key
hdp.SDFS_APIKEY.description=To use Secure Hadoop Data Encryption with your Hadoop cluster, please provide the credentials for your Object Storage account.
hdp.SDFS_LOCATION=Object Storage Datacenter Location
hdp.SDFS_LOCATION.description=Select Object Storage Datacenter Location
hdp.VPN_IP_ADDRESS=Public IP address of your on-premise gateway
hdp.VPN_IP_ADDRESS.description=Enter VPN Gateway IP Address
hdp.IKE_ENCRYPTION_ALG=Key-Exchange Encryption Algorithm
hdp.IKE_HASH_ALG=Data Integrity Hash Algorithm
hdp.IKE_DH_GROUP=Diffe-Hellman Group for IKE SA
hdp.AUTHENTICATION_MODE=Authentication Method
hdp.IKE_KEY_LIFETIME=Lifetime of IKE SA
hdp.CUST_SUBNETS=Customer Subnets
hdp.CUST_SUBNETS.description=Enter Subnet
hdp.ESP_ENCRYPTION_ALG=Encryption Algorithm for ESP
hdp.ESP_HASH_ALG=Hash Algorithm for ESP
hdp.ESP_KEY_LIFETIME=Key Lifetime for ESP (in seconds)
hdp.ESP_KEY_LIFETIME.description=Enter Key Lifetime for ESP
hdp.PRE_SHARED_SECRET=Pre-Shared Secret
hdp.PRE_SHARED_SECRET.description=Enter Pre-Shared Secret
hdp.PRE_SHARED_SECRET_CHK=Generate Pre-Shared Secret
hdp.PRE_SHARED_SECRET_CHK.description=Select checkbox to generate Pre-Shared Secret Key otherwise enter existing Pre-Shared Secret Key in below box
hdp.ESP_PFS=ESP Perfect Forward Secrecy

streams.dataNode=Worker Nodes
streams.dataNode.description=This cluster allows you to have from 1 up to n-1 worker nodes where n is the quota for your cluster. Click on Info Icon to learn more about Worker Nodes.
streams.dataNode.helpDescription=A worker node runs streams application jobs.
streams.masterNode=Master Node
streams.masterNode.description=This cluster requires 1 and only 1 master node. Click on Info Icon to learn more about Master Nodes.
streams.masterNode.helpDescription=A master node runs management services like Admin Console, Management API Service, etc. If the cluster has / provides access to the Internet, the master node is provisioned with a public interface.

#DataCenter
#dataCenter.1.name=From Softlayer Object

#PcmaeProvisioningService:---

pcmae.trial=Your trial has expired.
pcmae.avail=You do not have enough available nodes in your account.
pcmae.plugin=Service provider plugin not found.
pcmae.fail=Failed to create cluster :


#TransfersResource:--------

trans.location=TransLocation is not provided

#hortonworks-plugin :---

hdp.max=Maximum compute tier size is {0}, cannot provision {1} nodes.
hdp.min=Minimum cluster size is " + machineCount + ", cannot provision " + clusterSize + " nodes.
hdp.int=Internal error: compute tier (" + computeTierName + ") not found in cluster definition.

#Streams-plugin:-----------
stream.def=Cluster definition not found.
stream.max=You have reached the maximum number of clusters allowed for the Limited Preview
stream.max.size=Maximum compute tier size is " + maxComputeSize + ", cannot provision " + newComputeSize + " nodes.
stream.min=Minimum cluster size is " + machineCount + ", cannot provision " + clusterSize + " nodes.
stream.int=Internal error: compute tier (" + edgeTierName + ") not found in cluster definition.


#Index.jsp :---
cpe.dashboard.ibm=IBM
cpe.dashboard.cloud=Cloud
cpe.dashboard.about=About
cpe.dashboard.what=What is Cloud Computing
cpe.dashboard.why=Why IBM Cloud
cpe.dashboard.marketplace=Marketplace
cpe.dashboard.partners=Partners
cpe.dashboard.solutions=Solutions
cpe.dashboard.community=Community
cpe.dashboard.analytics=Cloud Analytics
cpe.dashboard.contact=Contact IBM
cpe.dashboard.privacy=Privacy
cpe.dashboard.terms=Terms of Use
cpe.dashboard.accessibility=Accessibility
cpe.dashboard.cookie=Cookie Preference
cpe.dashboard.logout=Logout